{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6CfGlYo9PS/5Z0kkGXPQm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedilyas/CASA-Crowd/blob/main/Nested_UNet_Densnet_Resnet_Dialted_Conv_MD_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ccdku4nHYe0Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class batchnorm_relu(nn.Module):\n",
        "    def __init__(self, in_c):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(in_c)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.bn(inputs)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class conv_block_nested(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch,stride=1):\n",
        "        super().__init__()\n",
        "        \"\"\" Convolutional layer \"\"\"\n",
        "        self.b1 = batchnorm_relu(in_ch)\n",
        "        self.c1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, stride=stride)\n",
        "        self.b2 = batchnorm_relu(mid_ch)\n",
        "        self.c2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        \"\"\" Shortcut Connection (Identity Mapping) \"\"\"\n",
        "\n",
        "\n",
        "        # Adjust the dimensions if the input channels or stride is different from the output\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.shortcut = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),nn.BatchNorm2d(out_ch))\n",
        "        else:\n",
        "          self.shortcut = nn.Conv2d(in_ch, out_ch, kernel_size=1, padding=0, stride=stride)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.b1(inputs)\n",
        "        # print(\"1\", x.shape)\n",
        "        x = self.c1(x)\n",
        "        # print(\"c1\", x.shape)\n",
        "        x = self.b2(x)\n",
        "        # print(\"3\", x.shape)\n",
        "        x = self.c2(x)\n",
        "        # print(\"c2\", x.shape)\n",
        "        s = self.shortcut(inputs)\n",
        "\n",
        "        skip = x + s\n",
        "        # print(\"skip\", skip.shape)\n",
        "        return skip\n",
        "\n",
        "\n",
        "class DDCB0_0(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB0_0, self).__init__()\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + in_ch, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 448, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 128, 1),nn.BatchNorm2d(128),nn.ReLU(True),nn.Dropout(0.2),\n",
        "                                   nn.Conv2d(128, 64, 3, padding=1, dilation=1),nn.BatchNorm2d(64),nn.ReLU(True),nn.Dropout(0.2),)\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 128, 1),nn.BatchNorm2d(128),nn.ReLU(True),nn.Dropout(0.2),\n",
        "                                   nn.Conv2d(128, 64, 3, padding=2, dilation=2),nn.BatchNorm2d(64),nn.ReLU(True),nn.Dropout(0.2),)\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 128, 1),nn.BatchNorm2d(128),nn.ReLU(True),nn.Dropout(0.2),\n",
        "                                   nn.Conv2d(128, 64, 3, padding=3, dilation=3),nn.BatchNorm2d(64),nn.ReLU(True),nn.Dropout(0.2),)\n",
        "\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, out_ch, 3, padding=4, dilation=4),nn.ReLU(True),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DDCB1(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB1, self).__init__()\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, in_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + in_ch, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 448, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 128, 1),nn.BatchNorm2d(128),nn.ReLU(True),nn.Dropout(0.2),\n",
        "                                   nn.Conv2d(128, 64, 3, padding=1, dilation=1),nn.BatchNorm2d(64),nn.ReLU(True),nn.Dropout(0.2),)\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 128, 1),nn.BatchNorm2d(128),nn.ReLU(True),nn.Dropout(0.2),\n",
        "                                   nn.Conv2d(128, 64, 3, padding=2, dilation=2),nn.BatchNorm2d(64),nn.ReLU(True),nn.Dropout(0.2),)\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 128, 1),nn.BatchNorm2d(128),nn.ReLU(True),nn.Dropout(0.2),\n",
        "                                   nn.Conv2d(128, 64, 3, padding=3, dilation=3),nn.BatchNorm2d(64),nn.ReLU(True),nn.Dropout(0.2),)\n",
        "\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, out_ch, 3, padding=4, dilation=4),nn.ReLU(True),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "\n",
        "# class conv_block_nested(nn.Module):\n",
        "#     def __init__(self, in_ch, mid_ch, out_ch):\n",
        "#         super(conv_block_nested, self).__init__()\n",
        "#         self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "#         self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "#         self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "#         self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "#         # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "#         # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "#         # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "#         # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1_raw = self.conv1(x)\n",
        "#         x1 = torch.cat([x, x1_raw], 1)\n",
        "#         x2_raw = self.conv2(x1)\n",
        "#         x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "#         x3_raw = self.conv3(x2)\n",
        "#         x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "#         output = self.conv4(x3)\n",
        "#         return output\n",
        "\n",
        "# class down_sample_Conv(nn.Module):\n",
        "#   def __init__(self, in_ch1, out_ch1):\n",
        "#     super(down_sample_Conv,self).__init__()\n",
        "#     self.pool = nn.Conv2d(in_ch1, out_ch1, kernel_size=3, stride=1, padding=1, dilation=2)\n",
        "#   def forward(self,x):\n",
        "#     x = self.pool(x)\n",
        "#     return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DDCB_5_64_64(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB_5_64_64, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        # print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "class DDCB_64_128_128(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB_64_128_128, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 128, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 256, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 256, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        # print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "class DDCB_128_256_256(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB_128_256_256, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 256, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 512, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 512, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        # print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "\n",
        "class DDCB_256_512_512(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB_256_512_512, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 512, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, 512, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 1024, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 1024, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        # x3_raw = self.conv3(x2)\n",
        "        # print('x3_raw', x3_raw.shape)\n",
        "        # x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        # print('x3', x3.shape)\n",
        "        # output = self.conv4(x3)\n",
        "        # # print('output', output.shape)\n",
        "        return x2"
      ],
      "metadata": {
        "id": "egM9GP2Q0GNN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NestedUNet(nn.Module):\n",
        "    def __init__(self, in_ch=5, out_ch=6):\n",
        "        super(NestedUNet, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv0_0 = DDCB_5_64_64(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = DDCB_64_128_128(filters[0], filters[1], filters[1])\n",
        "        self.conv2_0 = DDCB_128_256_256(filters[1], filters[2], filters[2])\n",
        "        self.conv3_0 = DDCB1(filters[2], filters[3], filters[3])\n",
        "        self.conv4_0 = DDCB1(filters[3], filters[4], filters[4])\n",
        "\n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x0_0 = self.conv0_0(x)\n",
        "        print(\"x0_0\",x0_0.shape)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        # print(\"x1_0\", x0_0.shape)\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\n",
        "        # print(\"x0_1\", x0_1.shape)\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        # print(\"x2_0\", x2_0.shape)\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        # print(\"x3_0\", x3_0.shape)\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        # print(\"x4_0\", x4_0.shape)\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\n",
        "\n",
        "        output = self.final(x0_4)\n",
        "        return output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DDCB(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        print('output', output.shape)\n",
        "        return output\n",
        ""
      ],
      "metadata": {
        "id": "htABMrrxY0GR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if __name__ == \"__main__\":\n",
        "    # Instantiate the NestedUNet model\n",
        "    model = NestedUNet().to(device)\n",
        "    # Generate a sample input tensor\n",
        "    input_tensor = torch.randn(1, 5, 320, 320).to(device)\n",
        "    output = model(input_tensor)\n",
        "    # print(output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7aSuomqY4AH",
        "outputId": "19fe7077-0ee7-444f-95ed-f73f17a976e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x torch.Size([1, 5, 320, 320])\n",
            "x1_raw torch.Size([1, 64, 320, 320])\n",
            "x1 torch.Size([1, 69, 320, 320])\n",
            "x2_raw torch.Size([1, 64, 320, 320])\n",
            "x2 torch.Size([1, 133, 320, 320])\n",
            "x3_raw torch.Size([1, 64, 320, 320])\n",
            "x3 torch.Size([1, 133, 320, 320])\n",
            "x0_0 torch.Size([1, 64, 320, 320])\n",
            "x torch.Size([1, 64, 160, 160])\n",
            "x1_raw torch.Size([1, 128, 160, 160])\n",
            "x1 torch.Size([1, 192, 160, 160])\n",
            "x2_raw torch.Size([1, 128, 160, 160])\n",
            "x2 torch.Size([1, 320, 160, 160])\n",
            "x3_raw torch.Size([1, 128, 160, 160])\n",
            "x3 torch.Size([1, 320, 160, 160])\n",
            "x torch.Size([1, 128, 80, 80])\n",
            "x1_raw torch.Size([1, 256, 80, 80])\n",
            "x1 torch.Size([1, 384, 80, 80])\n",
            "x2_raw torch.Size([1, 256, 80, 80])\n",
            "x2 torch.Size([1, 640, 80, 80])\n",
            "x3_raw torch.Size([1, 256, 80, 80])\n",
            "x3 torch.Size([1, 640, 80, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DDCB_64_128_128(nn.Module):\n",
        "    def __init__(self, in_ch=64, mid_ch=128, out_ch):\n",
        "        super(DDCB_64_128_128, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 128, 1), nn.ReLU(True), nn.Conv2d(128, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Instantiate the NestedUNet model\n",
        "    model = DDCB_64_128_128(64,128,128).to(device)\n",
        "    # Generate a sample input tensor\n",
        "    input_tensor = torch.randn(1,64,224,224).to(device)\n",
        "    output = model(input_tensor)\n",
        "    print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "_jDO9-_iZKD3",
        "outputId": "f1358747-cc4b-4c44-bca8-c42a313c1bf8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x torch.Size([1, 64, 224, 224])\n",
            "x1_raw torch.Size([1, 128, 224, 224])\n",
            "x1 torch.Size([1, 192, 224, 224])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-2eec6c602332>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Generate a sample input tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-2eec6c602332>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_raw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx2_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x2_raw'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_raw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 128, 1, 1], expected input[1, 192, 224, 224] to have 128 channels, but got 192 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DDCB_64_128_128(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB_64_128_128, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, in_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128 + 64, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 256, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        # print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Instantiate the NestedUNet model\n",
        "    model = DDCB_64_128_128(64,128,128).to(device)\n",
        "    # Generate a sample input tensor\n",
        "    input_tensor = torch.randn(1,64,224,224).to(device)\n",
        "    output = model(input_tensor)\n",
        "    print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI4EU8ZwtH7h",
        "outputId": "c76ba327-9ddd-4cbf-aab4-a7fd26b1a4db"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x torch.Size([1, 64, 224, 224])\n",
            "x1_raw torch.Size([1, 64, 224, 224])\n",
            "x1 torch.Size([1, 128, 224, 224])\n",
            "x2_raw torch.Size([1, 128, 224, 224])\n",
            "x2 torch.Size([1, 256, 224, 224])\n",
            "x3_raw torch.Size([1, 128, 224, 224])\n",
            "x3 torch.Size([1, 320, 224, 224])\n",
            "torch.Size([1, 128, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DDCB_64_128_128(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB_64_128_128, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 128, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 256, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 256, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        # print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Instantiate the NestedUNet model\n",
        "    model = DDCB_64_128_128(64,128,128).to(device)\n",
        "    # Generate a sample input tensor\n",
        "    input_tensor = torch.randn(1,64,224,224).to(device)\n",
        "    output = model(input_tensor)\n",
        "    print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lanziOoTy38J",
        "outputId": "eb892e7f-d407-493b-b076-ac0160d1e935"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x torch.Size([1, 64, 224, 224])\n",
            "x1_raw torch.Size([1, 128, 224, 224])\n",
            "x1 torch.Size([1, 192, 224, 224])\n",
            "x2_raw torch.Size([1, 128, 224, 224])\n",
            "x2 torch.Size([1, 320, 224, 224])\n",
            "x3_raw torch.Size([1, 128, 224, 224])\n",
            "x3 torch.Size([1, 320, 224, 224])\n",
            "torch.Size([1, 128, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DDCB_128_256_256(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB_128_256_256, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 256, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 512, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 512, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        x3_raw = self.conv3(x2)\n",
        "        print('x3_raw', x3_raw.shape)\n",
        "        x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        print('x3', x3.shape)\n",
        "        output = self.conv4(x3)\n",
        "        # print('output', output.shape)\n",
        "        return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Instantiate the NestedUNet model\n",
        "    model = DDCB_128_256_256(128,256,256).to(device)\n",
        "    # Generate a sample input tensor\n",
        "    input_tensor = torch.randn(1,128,224,224).to(device)\n",
        "    output = model(input_tensor)\n",
        "    print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARdFvCJkzlyP",
        "outputId": "fca48083-8506-4360-a25e-d16ff8790655"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x torch.Size([1, 128, 224, 224])\n",
            "x1_raw torch.Size([1, 256, 224, 224])\n",
            "x1 torch.Size([1, 384, 224, 224])\n",
            "x2_raw torch.Size([1, 256, 224, 224])\n",
            "x2 torch.Size([1, 640, 224, 224])\n",
            "x3_raw torch.Size([1, 256, 224, 224])\n",
            "x3 torch.Size([1, 640, 224, 224])\n",
            "torch.Size([1, 256, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DDCB_256_512_512(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DDCB_256_512_512, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_ch, mid_ch, 1), nn.ReLU(True), nn.Conv2d(mid_ch, out_ch, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 512, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, 512, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 1024, mid_ch, 1), nn.ReLU(True),nn.Conv2d(mid_ch, out_ch, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 1024, out_ch, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "        # self.conv1 = nn.Sequential(nn.Conv2d(in_ch, 256, 1), nn.ReLU(True), nn.Conv2d(256, 64, 3, padding=1,dilation=1),nn.ReLU(True))\n",
        "        # self.conv2 = nn.Sequential(nn.Conv2d(in_ch + 64, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=2, dilation=2), nn.ReLU(True))\n",
        "        # self.conv3 = nn.Sequential(nn.Conv2d(in_ch + 128, 256, 1), nn.ReLU(True),nn.Conv2d(256, 64, 3, padding=3, dilation=3), nn.ReLU(True))\n",
        "        # self.conv4 = nn.Sequential(nn.Conv2d(in_ch + 128, 512, 3, padding=4, dilation=4), nn.ReLU(True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('x', x.shape)\n",
        "        x1_raw = self.conv1(x)\n",
        "        print('x1_raw', x1_raw.shape)\n",
        "        x1 = torch.cat([x, x1_raw], 1)\n",
        "        print('x1', x1.shape)\n",
        "        x2_raw = self.conv2(x1)\n",
        "        print('x2_raw', x2_raw.shape)\n",
        "        x2 = torch.cat([x, x1_raw, x2_raw], 1)\n",
        "        print('x2', x2.shape)\n",
        "        # x3_raw = self.conv3(x2)\n",
        "        # print('x3_raw', x3_raw.shape)\n",
        "        # x3 = torch.cat([x, x2_raw, x3_raw], 1)\n",
        "        # print('x3', x3.shape)\n",
        "        # output = self.conv4(x3)\n",
        "        # # print('output', output.shape)\n",
        "        return x2\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Instantiate the NestedUNet model\n",
        "    model = DDCB_256_512_512(256,512,512).to(device)\n",
        "    # Generate a sample input tensor\n",
        "    input_tensor = torch.randn(1,256,512,512).to(device)\n",
        "    output = model(input_tensor)\n",
        "    print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxlfUA4c4RnU",
        "outputId": "26188919-d337-4ab8-adb1-fef38e8bc803"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x torch.Size([1, 256, 512, 512])\n",
            "x1_raw torch.Size([1, 512, 512, 512])\n",
            "x1 torch.Size([1, 768, 512, 512])\n",
            "x2_raw torch.Size([1, 512, 512, 512])\n",
            "x2 torch.Size([1, 1280, 512, 512])\n",
            "torch.Size([1, 1280, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAKEfIuI9pwi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}